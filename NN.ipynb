{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2TR1koO7pECs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Used to scale the data features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "# Used to calculate AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of headings for the data.\n",
        "\n",
        "# Read in the data from the Stanford website.\n",
        "loan = pd.read_csv('Loan_default.csv')\n",
        "loan.columns"
      ],
      "metadata": {
        "id": "gsPQRow1pSs7",
        "outputId": "dfb2710f-b1ae-4fa9-b5f5-d9eaafca8927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['LoanID', 'Age', 'Income', 'LoanAmount', 'CreditScore',\n",
              "       'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm',\n",
              "       'DTIRatio', 'Education', 'EmploymentType', 'MaritalStatus',\n",
              "       'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner',\n",
              "       'Default'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loan_data = loan.drop(['LoanID','Education','EmploymentType', 'MaritalStatus',\n",
        "       'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner'],axis=1)"
      ],
      "metadata": {
        "id": "aRMFQ3w1z1m_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a separate dataframe containing the features (X) and response(Y).\n",
        "X = loan_data.iloc[:,:-1]\n",
        "  # drops the last column of the dataframe that contains the iris type (response).\n",
        "Y = loan.iloc[:,-1]"
      ],
      "metadata": {
        "id": "QLCGmTl6tpBA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Gz3JNc__0dgP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = X.shape[0]\n",
        "k = X.shape[1]\n",
        "b = 1\n",
        "l = int(n//b) -1\n",
        "p = 50\n",
        "h = 4\n",
        "o = 1\n",
        "r = 0.0001\n",
        "h_weight = np.zeros((p,k,h))\n",
        "o_weight = np.zeros((p,h,o))\n",
        "h_bias = np.zeros((p,1,h))\n",
        "o_bias = np.zeros((p,1,o))\n",
        "h_node = np.zeros((l,h))\n",
        "o_node = np.zeros((l,o))"
      ],
      "metadata": {
        "id": "FjL7LAE_uFq5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "for m in range(0,p,1):\n",
        "  if m == 1:\n",
        "    h_weight_init = np.random.rand(k,h)\n",
        "    o_weight_init = np.random.rand(h,o)\n",
        "    h_bias_init = np.random.rand(1,h)\n",
        "    o_bias_init = np.random.rand(1,o)\n",
        "    h_weight[0,:,:] = h_weight_init\n",
        "    o_weight[0,:,:] = o_weight_init\n",
        "    h_bias[0,:,:] = h_bias_init\n",
        "    o_bias[0,:,:] = o_bias_init\n",
        "  else:\n",
        "    h_weight[m,:,:] = h_weight[m-1,:,:]\n",
        "    o_weight[m,:,:] = o_weight[m-1,:,:]\n",
        "    h_bias[m,:,:] = h_bias[m-1,:,:]\n",
        "    o_bias[m,:,:] = o_bias[m-1,:,:]\n",
        "  for i in range(0,l,1):\n",
        "    pos = i * b\n",
        "    X_batch = X[pos:pos+b,:]\n",
        "    Y_batch = Y[pos:pos+b]\n",
        "    e = np.zeros((b,5))\n",
        "    for j in range(0,h):\n",
        "      h_node[i,j] = np.sum(np.dot(X_batch,h_weight[m,:,j]) + h_bias[m,:,j])\n",
        "      h_node[i,j] = 1/(1+np.exp(-h_node[i,j]))\n",
        "    o_node[i] = np.sum(np.dot(h_node[i,:],o_weight[m,:,:]) + o_bias[m,:,:])\n",
        "    o_node[i] = 1/(1+np.exp(-o_node[i]))\n",
        "    e[:,0] = o_node[i]\n",
        "    e[:,1] = Y_batch\n",
        "    if e[:,0] > 0.5:\n",
        "      e[:,2] = 1\n",
        "    else:\n",
        "      e[:,2] = 0\n",
        "    if e[:,2] == e[:,1]:\n",
        "      e[:,4] = 1\n",
        "    else:\n",
        "      e[:,4] = 0\n",
        "    e[:,3] = (e[:,1] - e[:,0])**2\n",
        "    error = np.sum(e[:,3])\n",
        "    if i < l - 1:\n",
        "      delta_bp = o_node[i] * (1 - o_node[i]) * (Y_batch - o_node[i])\n",
        "      delta_h = h_node[i] * (1 - h_node[i]) * np.dot(o_weight[m,:,:],delta_bp)\n",
        "      o_weight[m,:,:] = o_weight[m,:,:] + r * delta_bp.ravel() * h_node[i].reshape(-1,1)\n",
        "      o_bias[m,:,:] = o_bias[m,:,:] + r * delta_bp.ravel()\n",
        "      h_weight[m,:,:] = h_weight[m,:,:] + (r * delta_h.reshape(-1,1) * X_batch).T\n",
        "      h_bias[m,:,:] = h_bias[m,:,:] + r * delta_h.reshape(-1,1).T\n",
        "    # evaluate final result accuracy\n",
        "    e = np.zeros((n,5))\n",
        "    h_node_all = np.zeros((n,h))\n",
        "    o_node_all = np.zeros((n,o))\n",
        "  for i in range(0,n,1):\n",
        "    h_node_all[i,:] = np.sum(np.dot(X[i,:],h_weight[m,:,:]) + h_bias[m,:,:])\n",
        "    h_node_all[i,:] = 1/(1+np.exp(-h_node_all[i,:]))\n",
        "    o_node_all[i] = np.sum(np.dot(h_node_all[i,:],o_weight[m,:,:]) + o_bias[m,:,:])\n",
        "    o_node_all[i] = 1/(1+np.exp(-o_node_all[i]))\n",
        "    e[i,0] = o_node_all[i]\n",
        "    e[i,1] = Y[i]\n",
        "    if e[i,0] > 0.5:\n",
        "      e[i,2] = 1\n",
        "    else:\n",
        "      e[i,2] = 0\n",
        "    if e[i,2] == e[i,1]:\n",
        "      e[i,4] = 1\n",
        "    else:\n",
        "      e[i,4] = 0\n",
        "    e[i,3] = (e[i,1] - e[i,0])**2\n",
        "  error_all = np.sum(e[:,3])\n",
        "  accuracy = np.sum(e[:,4])/n\n",
        "  print(\"Epoch:\",m)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print(\"Total Error:\", error)\n",
        "print(int(time.time() - start_time)//60)"
      ],
      "metadata": {
        "id": "Sl_5GWn0umt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}